{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-27T08:50:36.984155Z","iopub.status.busy":"2022-03-27T08:50:36.983423Z","iopub.status.idle":"2022-03-27T08:50:37.021801Z","shell.execute_reply":"2022-03-27T08:50:37.020663Z","shell.execute_reply.started":"2022-03-27T08:50:36.984061Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pysentimiento in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (0.3.2)\n","Requirement already satisfied: emoji<2.0.0,>=1.6.1 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pysentimiento) (1.7.0)\n","Requirement already satisfied: sklearn<0.1,>=0.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pysentimiento) (0.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.11.3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pysentimiento) (4.17.0)\n","Requirement already satisfied: datasets<2.0.0,>=1.13.3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pysentimiento) (1.18.4)\n","Requirement already satisfied: torch<2.0.0,>=1.9.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pysentimiento) (1.11.0)\n","Requirement already satisfied: dill in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.3.4)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (4.63.1)\n","Requirement already satisfied: xxhash in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.0.0)\n","Requirement already satisfied: responses<0.19 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.4.0)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.21.5)\n","Requirement already satisfied: packaging in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (21.3)\n","Requirement already satisfied: multiprocess in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.70.12.2)\n","Requirement already satisfied: pandas in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (7.0.0)\n","Requirement already satisfied: aiohttp in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.8.1)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2.27.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2022.2.0)\n","Requirement already satisfied: pyyaml in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.13.3->pysentimiento) (4.1.1)\n","Requirement already satisfied: filelock in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.13.3->pysentimiento) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from packaging->datasets<2.0.0,>=1.13.3->pysentimiento) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from requests>=2.19.0->datasets<2.0.0,>=1.13.3->pysentimiento) (2021.10.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from requests>=2.19.0->datasets<2.0.0,>=1.13.3->pysentimiento) (1.26.9)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from requests>=2.19.0->datasets<2.0.0,>=1.13.3->pysentimiento) (3.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from requests>=2.19.0->datasets<2.0.0,>=1.13.3->pysentimiento) (2.0.12)\n","Requirement already satisfied: scikit-learn in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from sklearn<0.1,>=0.0->pysentimiento) (1.0.2)\n","Requirement already satisfied: colorama in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from tqdm>=4.62.1->datasets<2.0.0,>=1.13.3->pysentimiento) (0.4.4)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from transformers<5.0.0,>=4.11.3->pysentimiento) (2022.3.15)\n","Requirement already satisfied: sacremoses in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from transformers<5.0.0,>=4.11.3->pysentimiento) (0.0.49)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from transformers<5.0.0,>=4.11.3->pysentimiento) (0.11.6)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (21.4.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.7.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2021.3)\n","Requirement already satisfied: six>=1.5 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (1.16.0)\n","Requirement already satisfied: joblib in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.11.3->pysentimiento) (1.1.0)\n","Requirement already satisfied: click in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.11.3->pysentimiento) (8.0.4)\n","Requirement already satisfied: scipy>=1.1.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (1.8.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\leona\\anaconda3\\envs\\racism_leo\\lib\\site-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (3.1.0)\n"]}],"source":["!pip install pysentimiento"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-03-27T08:50:37.024919Z","iopub.status.busy":"2022-03-27T08:50:37.024173Z"},"trusted":true},"outputs":[],"source":["# %%\n","\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n","                          DataCollatorWithPadding, Trainer, TrainingArguments,\n","                          pipeline)\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","if Path(\"/kaggle\").exists():\n","    input_path = Path(\"/kaggle\") / \"input\" / \"racism\"\n","    output_path = Path(\"/kaggle\") / \"working\"\n","    tmp_path = output_path / \"tmp\"\n","    models_path = output_path / \"artifacts\"\n","    models_path.mkdir(exist_ok=True, parents=True)\n","    tmp_path.mkdir(exist_ok=True, parents=True)\n","else:\n","    data_path = Path(\"data\")\n","    input_path = data_path / \"split\"\n","    tmp_path = data_path / \"tmp\"\n","    tmp_path.mkdir(exist_ok=True, parents=True)\n","    models_path = Path(\"models\") / \"artifacts\"\n","    output_path = models_path\n","\n","def replace_quotes(x: pd.Series) -> pd.Series:\n","    \"\"\"\n","    Replace quotes with the word \"cita\" to keep some info\n","    \"\"\"\n","    regex = '\"([^\"]*)\"'\n","    x = x.str.replace(regex, \"cita\")\n","\n","    return x\n","\n","def process_text(df):\n","    \"\"\"\n","    Some preprocessing of emojis and quotes\n","    \"\"\"\n","    df[\"message\"] = df[\"message\"].apply(preprocess_tweet)\n","    #df[\"message\"] = replace_quotes(df[\"message\"])\n","    return df\n","\n","df = pd.read_csv( data_path / \"labels_racism_regression_train.txt\", delimiter=\"|\")\n","df2 = pd.read_csv(data_path / \"labels_racism.csv\", sep=\"|\")\n","df[\"clas_label\"] = df2.label\n","messages = df.message.unique()\n","np.random.seed(42)\n","messages_train = np.random.choice(\n","    messages, size=int(len(messages) * 0.6), replace=False)\n","\n","\n","df_train = df.loc[df.message.isin(messages_train)].reset_index(drop=True)\n","df_test = df.loc[~df.message.isin(messages_train)].reset_index(drop=True)\n","\n","\n","# %% Preprocess data a bit\n","train_df = process_text(df_train)\n","test_df = process_text(df_test)\n","\n","train_df = train_df.rename(columns={\n","    'message': 'text'})\n","train_df.to_csv(tmp_path / \"labels_racism_train.csv\", index=False)\n","\n","test_df = test_df.rename(columns={\n","    'message': 'text'})\n","test_df.to_csv(tmp_path / \"labels_racism_test.csv\", index=False)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["5565"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","# MODEL_NAME = \"pysentimiento/robertuito-sentiment-analysis\"\n","MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n","#MODEL_NAME = \"PlanTL-GOB-ES/roberta-large-bne\"\n","\n","\n","# %% Load tokenizer and train\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME, num_labels=1, ignore_mismatched_sizes=True\n",")\n","\n","\n","def preprocess_function(examples):\n","    # txt = [x.replace(\"gitano\", \"negro\") for x in txt]\n","    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n","\n","\n","# %% Load ready for hf\n","dataset = load_dataset(path=str(tmp_path), data_files={\n","    'train': 'labels_racism_train.csv',\n","    'validation': 'labels_racism_test.csv'\n","    }\n",")\n","\n","\n","\n","# %% Preprocess data\n","tokenized_data = dataset.map(preprocess_function, batched=True)\n","\n","\n","# %% Train model\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","\n","# %% Predict on validation\n","preds = trainer.predict(tokenized_data[\"validation\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_preds = trainer.predict(tokenized_data[\"train\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def threshold_optimisation(preds, y, n_thresholds=200):\n","\n","    df_thresholds = pd.DataFrame()\n","    for threshold in np.linspace(0, 1, n_thresholds):\n","        df_thresholds = df_thresholds.append(\n","            pd.DataFrame({\n","                \"threshold\": [threshold],\n","                \"f1\": [f1_score(preds > threshold, y)]\n","            })\n","        )\n","\n","    return df_thresholds.sort_values(\"f1\", ascending=False).head(1).threshold.to_list()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y = train_df.clas_label\n","y_preds = sigmoid(train_preds.predictions[:, 1] - train_preds.predictions[:, 0])\n","opt_threshold = threshold_optimisation(y_preds, y)\n","opt_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_valid = preds.label_ids\n","y_valid_preds = sigmoid(preds.predictions[:, 1] - preds.predictions[:, 0])\n","opt_threshold_valid = threshold_optimisation(y_valid_preds, y_valid)\n","opt_threshold_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f1_score(preds.label_ids, y_valid_preds > opt_threshold))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f1_score(train_preds.label_ids, y_preds > opt_threshold))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(models_path)\n","tokenizer.save_pretrained(models_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(output_path)\n","tokenizer.save_pretrained(output_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df_out = pd.read_csv(input_path / \"labels_racism_test.txt\", delimiter=\"|\").query(\"label != 'unknown'\")\n","test_df_out.assign(racist_score=y_valid_preds).to_csv(output_path / \"hf_v1_validation.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# %% Create pipeline\n","p = pipeline(\n","    \"text-classification\", model=str(models_path), tokenizer=str(models_path))\n","\n","# %% Save pipeline\n","p.save_pretrained(models_path / \"pipe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"full\"],\n","    eval_dataset=tokenized_data[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(output_path)\n","tokenizer.save_pretrained(output_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
