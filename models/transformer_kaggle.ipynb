{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-27T08:50:36.984155Z","iopub.status.busy":"2022-03-27T08:50:36.983423Z","iopub.status.idle":"2022-03-27T08:50:37.021801Z","shell.execute_reply":"2022-03-27T08:50:37.020663Z","shell.execute_reply.started":"2022-03-27T08:50:36.984061Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-27T08:50:37.024919Z","iopub.status.busy":"2022-03-27T08:50:37.024173Z"},"trusted":true},"outputs":[],"source":["# %%\n","\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n","                          DataCollatorWithPadding, Trainer, TrainingArguments,\n","                          pipeline)\n","\n","if Path(\"/kaggle\").exists():\n","    input_path = Path(\"/kaggle\") / \"input\" / \"racism\"\n","    output_path = Path(\"/kaggle\") / \"working\"\n","    tmp_path = output_path / \"tmp\"\n","    models_path = output_path / \"artifacts\"\n","    models_path.mkdir(exist_ok=True, parents=True)\n","    tmp_path.mkdir(exist_ok=True, parents=True)\n","else:\n","    data_path = Path(\"data\")\n","    input_path = data_path / \"split\"\n","    tmp_path = data_path / \"tmp\"\n","    tmp_path.mkdir(exist_ok=True, parents=True)\n","    models_path = Path(\"models\") / \"artifacts\"\n","    output_path = models_path\n","\n","\n","train_df = pd.read_csv(input_path / \"labels_racism_train.txt\", delimiter=\"|\")\n","test_df = pd.read_csv(input_path / \"labels_racism_test.txt\", delimiter=\"|\")\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","# MODEL_NAME = \"pysentimiento/robertuito-sentiment-analysis\"\n","MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n","\n","\n","# %% Load tokenizer and train\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME, num_labels=2, ignore_mismatched_sizes=True\n",")\n","\n","\n","def preprocess_function(examples):\n","    # txt = [x.replace(\"gitano\", \"negro\") for x in txt]\n","    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n","\n","\n","# %% Load data\n","label_key = {'non-racist': 0, 'racist': 1, 'unknown': 2}\n","\n","train_df[\"label\"] = [label_key[item] for item in train_df.label]\n","test_df[\"label\"] = [label_key[item] for item in test_df.label]\n","\n","\n","train_prep = train_df.query(\"label != 2\").drop(columns='labeller_id', axis=1).rename(columns={\n","    'message': 'text'})\n","train_prep.to_csv(tmp_path / 'labels_racism_train.csv', index=False)\n","test_prep = test_df.query(\"label != 2\").drop(columns='labeller_id', axis=1).rename(columns={\n","    'message': 'text'})\n","test_prep.to_csv(tmp_path / 'labels_racism_test.csv', index=False)\n","\n","pd.concat([train_prep, test_prep]).to_csv(tmp_path / 'labels_racism_full.csv', index=False)\n","\n","# %% Load ready for hf\n","dataset = load_dataset(path=str(tmp_path), data_files={\n","    'train': 'labels_racism_train.csv',\n","    'validation': 'labels_racism_test.csv',\n","    'full': 'labels_racism_full.csv',\n","    }\n",")\n","\n","\n","# %% Preprocess data\n","tokenized_data = dataset.map(preprocess_function, batched=True)\n","\n","\n","# %% Train model\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","\n","# %% Predict on validation\n","preds = trainer.predict(tokenized_data[\"validation\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_preds = trainer.predict(tokenized_data[\"train\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def threshold_optimisation(preds, y, n_thresholds=200):\n","\n","    df_thresholds = pd.DataFrame()\n","    for threshold in np.linspace(0, 1, n_thresholds):\n","        df_thresholds = df_thresholds.append(\n","            pd.DataFrame({\n","                \"threshold\": [threshold],\n","                \"f1\": [f1_score(preds > threshold, y)]\n","            })\n","        )\n","\n","    return df_thresholds.sort_values(\"f1\", ascending=False).head(1).threshold.to_list()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y = train_preds.label_ids\n","y_preds = sigmoid(train_preds.predictions[:, 1] - train_preds.predictions[:, 0])\n","opt_threshold = threshold_optimisation(y_preds, y)\n","opt_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_valid = preds.label_ids\n","y_valid_preds = sigmoid(preds.predictions[:, 1] - preds.predictions[:, 0])\n","opt_threshold_valid = threshold_optimisation(y_valid_preds, y_valid)\n","opt_threshold_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f1_score(preds.label_ids, y_valid_preds > opt_threshold))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f1_score(train_preds.label_ids, y_preds > opt_threshold))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(models_path)\n","tokenizer.save_pretrained(models_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(output_path)\n","tokenizer.save_pretrained(output_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df_out = pd.read_csv(input_path / \"labels_racism_test.txt\", delimiter=\"|\").query(\"label != 'unknown'\")\n","test_df_out.assign(racist_score=y_valid_preds).to_csv(output_path / \"hf_v1_validation.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# %% Create pipeline\n","p = pipeline(\n","    \"text-classification\", model=str(models_path), tokenizer=str(models_path))\n","\n","# %% Save pipeline\n","p.save_pretrained(models_path / \"pipe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"full\"],\n","    eval_dataset=tokenized_data[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(output_path)\n","tokenizer.save_pretrained(output_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
